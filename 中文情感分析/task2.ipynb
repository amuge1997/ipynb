{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先加载必用的库\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba # 结巴分词\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim加载预训练中文分词embedding, 有可能需要等待1-2分钟\n",
    "cn_model = KeyedVectors.load_word2vec_format('embeddings/sgns.zhihu.bigram', \n",
    "                                             binary=False, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只使用前20000个词\n",
    "num_words = 50000\n",
    "embedding_dim = 300\n",
    "\n",
    "\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens_(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text\n",
    "def reverse_tokens(tokens_list):\n",
    "    revers_list = []\n",
    "    for i in tokens_list:\n",
    "        text = reverse_tokens_(i)\n",
    "        revers_list.append(text)\n",
    "    return revers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长长的list，其中含有4000个小list，对应每一条评价\n",
    "def to_tokens(train_texts_orig):\n",
    "    train_tokens = []\n",
    "    for text in train_texts_orig:\n",
    "        # 去掉标点\n",
    "        text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "        # 结巴分词\n",
    "        cut = jieba.cut(text)\n",
    "        # 结巴分词的输出结果为一个生成器\n",
    "        # 把生成器转换为list\n",
    "        cut_list = [ i for i in cut ]\n",
    "        for i, word in enumerate(cut_list):\n",
    "            try:\n",
    "                # 将词转换为索引index\n",
    "                cut_list[i] = cn_model.vocab[word].index\n",
    "            except KeyError:\n",
    "                # 如果词不在字典中，则输出0\n",
    "                cut_list[i] = 0\n",
    "        train_tokens.append(cut_list)\n",
    "    return train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls = to_tokens(['我喜欢这个女孩'])\n",
    "# print(ls)\n",
    "# ls = reverse_tokens(ls)\n",
    "# print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.173 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    8,    53,   145,  1862,     8, 48860,  7546,   112,  6981,\n",
       "         193,     1,    42,   254,   938,  4083,    89,   609, 48860,\n",
       "           4,   167,    24,   502,     1, 48860,  1862,  2345,    42,\n",
       "           1,   400,   107,     4,    16,     0, 13131,     4,   167,\n",
       "          24,   502,     1, 13131,   400,  1002,   667,    47,     1,\n",
       "        1737,  7546,    27, 38895,  4160,    11,    20,     0,     6,\n",
       "         130,  8742, 34809,     8,  1737,   195,     4,  3800,   377,\n",
       "       49791,     1,    42,    16,   319,  1261,  3384,  1046])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "with open(r'测试六类情感与两类情感的映射关系数据emotion/train.txt', encoding='utf8') as fp:\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    lines = fp.readlines()\n",
    "    for i in lines:\n",
    "        i = eval(i)\n",
    "        data_x.append(i['content'])\n",
    "        data_y.append(i['label'])\n",
    "\n",
    "train_tokens = to_tokens(data_x)\n",
    "lenght = [len(i) for i in train_tokens]\n",
    "\n",
    "max_tokens = np.mean(lenght) + 2 * np.std(lenght)\n",
    "max_tokens = int(max_tokens)\n",
    "\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')\n",
    "train_pad[ train_pad>=num_words ] = 0\n",
    "train_pad[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用tensorflow的keras接口来建模\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = 'sentiment_checkpoint.keras'\n",
    "# 尝试加载已训练模型\n",
    "\n",
    "# 用LSTM对样本进行分类\n",
    "model = Sequential()\n",
    "# 模型第一层为embedding\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    # input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(LSTM(units=32, return_sequences=False))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         15000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         186880    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,208,033\n",
      "Trainable params: 208,033\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# 我们来看一下模型的结构，一共90k左右可训练的变量\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06481326],\n",
       "       [0.51183647],\n",
       "       [0.34630623],\n",
       "       ...,\n",
       "       [0.46879902],\n",
       "       [0.07427481],\n",
       "       [0.57198983]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.predict(train_pad)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出\n",
    "data_x_reverse = reverse_tokens(train_pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签: surprise\n",
      "原句: 刚才同事给男朋友打电话说：今天愚人节，咱们晚上去吃鱼吧！\n",
      "复原:                                                         刚才同事给男朋友打电话说：今天愚人节咱们晚上去吃鱼吧\n",
      "概率: 0.31129318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31728"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 208\n",
    "print('标签:', data_y[i])\n",
    "print('原句:', data_x[i])\n",
    "print('复原:', data_x_reverse[i])\n",
    "print('概率:', result[i, 0])\n",
    "len(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 31, 33, 101, 183, 205, 208, 216, 220, 283, 350, 448, 498, 605, 612, 619, 665, 683, 729, 739, 788, 795, 837, 881, 904, 911, 940, 1027, 1108, 1137, 1233, 1346, 1404, 1414, 1426, 1430, 1443, 1546, 1650, 1677, 1689, 1710, 1820, 1825, 1831, 1843, 1848, 1863, 1896, 1913, 1929, 2040, 2062, 2113, 2117, 2179, 2229, 2381, 2392, 2411, 2435, 2469, 2487, 2488, 2506, 2508, 2514, 2656, 2707, 2833, 2836, 2879, 2900, 2905, 2912, 2981, 2995, 2999, 3010, 3014, 3077, 3086, 3127, 3142, 3158, 3175, 3217, 3222, 3238, 3252, 3263, 3346, 3440, 3496, 3528, 3586, 3641, 3716, 3756, 3770, 3810, 3832, 3839, 3899, 3974, 3990, 4020, 4137, 4142, 4155, 4198, 4221, 4226, 4228, 4232, 4298, 4344, 4349, 4374, 4564, 4606, 4623, 4628, 4678, 4727, 4837, 4884, 4941, 5000, 5037, 5102, 5119, 5143, 5151, 5222, 5234, 5275, 5368, 5444, 5457, 5488, 5538, 5648, 5722, 5758, 5763, 5782, 5784, 5838, 5852, 5870, 5893, 5917, 5999, 6013, 6067, 6083, 6094, 6169, 6175, 6204, 6205, 6248, 6260, 6267, 6287, 6292, 6298, 6366, 6400, 6413, 6436, 6458, 6538, 6551, 6562, 6594, 6676, 6684, 6697, 6824, 6854, 6862, 6885, 6891, 6928, 7158, 7245, 7347, 7356, 7411, 7441, 7449, 7497, 7517, 7563, 7586, 7642, 7670, 7676, 7702, 7728, 7738, 7865, 8078, 8167, 8301, 8403, 8530, 8598, 8689, 8728, 8786, 8789, 8796, 8803, 8812, 8820, 8834, 8846, 8891, 8928, 8962, 9008, 9011, 9036, 9049, 9095, 9102, 9196, 9236, 9269, 9275, 9298, 9371, 9495, 9512, 9573, 9619, 9649, 9676, 9719, 9746, 9777, 9860, 9861, 9904, 9939, 9967, 10064, 10090, 10093, 10112, 10157, 10183, 10195, 10266, 10268, 10299, 10303, 10326, 10329, 10348, 10352, 10366, 10428, 10448, 10490, 10562, 10564, 10624, 10635, 10749, 10786, 10790, 10804, 10806, 10817, 10822, 10854, 10870, 10886, 10896, 10911, 10926, 10930, 10948, 10951, 10977, 10995, 11118, 11119, 11142, 11254, 11304, 11312, 11324, 11326, 11331, 11341, 11417, 11479, 11532, 11766, 11785, 11906, 11937, 11938, 12046, 12053, 12079, 12093, 12126, 12165, 12244, 12261, 12263, 12320, 12353, 12362, 12388, 12447, 12614, 12689, 12696, 12699, 12741, 12781, 12806, 13092, 13101, 13179, 13230, 13300, 13332, 13483, 13510, 13513, 13520, 13582, 13601, 13629, 13703, 13722, 13938, 13979, 14076, 14168, 14240, 14254, 14307, 14323, 14327, 14336, 14428, 14432, 14508, 14530, 14556, 14628, 14646, 14650, 14705, 14725, 14753, 14790, 14793, 14796, 14797, 14812, 14821, 14844, 14868, 14894, 14906, 14939, 14954, 14982, 15056, 15065, 15070, 15101, 15132, 15170, 15226, 15251, 15335, 15341, 15376, 15385, 15443, 15464, 15467, 15609, 15646, 15672, 15724, 15736, 15830, 15875, 15894, 15958, 16062, 16069, 16121, 16165, 16225, 16311, 16338, 16364, 16368, 16404, 16457, 16469, 16495, 16496, 16505, 16586, 16658, 16660, 16759, 16770, 16789, 16814, 16832, 16875, 16923, 16952, 17002, 17018, 17029, 17151, 17159, 17177, 17206, 17214, 17280, 17284, 17314, 17404, 17440, 17461, 17467, 17489, 17514, 17543, 17610, 17613, 17622, 17626, 17637, 17690, 17817, 17852, 17857, 17860, 17922, 17924, 17931, 17985, 17992, 18039, 18070, 18207, 18248, 18280, 18282, 18301, 18338, 18371, 18431, 18440, 18492, 18517, 18523, 18529, 18571, 18576, 18867, 18977, 19044, 19066, 19099, 19161, 19222, 19245, 19264, 19305, 19341, 19344, 19367, 19368, 19373, 19388, 19461, 19478, 19510, 19533, 19548, 19655, 19669, 19702, 19734, 19751, 19774, 19836, 19871, 19882, 19886, 19890, 19913, 20003, 20053, 20135, 20160, 20262, 20263, 20281, 20327, 20333, 20470, 20501, 20522, 20563, 20569, 20589, 20642, 20692, 20763, 20769, 20782, 20788, 20836, 20863, 20889, 20947, 20982, 21037, 21155, 21167, 21193, 21290, 21296, 21344, 21385, 21401, 21459, 21502, 21504, 21524, 21542, 21664, 21722, 21828, 21921, 21953, 21954, 21998, 22019, 22061, 22080, 22089, 22171, 22250, 22276, 22337, 22366, 22397, 22408, 22420, 22437, 22455, 22467, 22473, 22532, 22542, 22552, 22608, 22610, 22674, 22684, 22735, 22737, 22753, 22851, 22866, 22901, 22908, 22916, 22927, 22930, 22931, 22958, 22979, 23007, 23191, 23221, 23329, 23356, 23408, 23419, 23423, 23454, 23467, 23494, 23572, 23604, 23627, 23632, 23695, 23737, 23810, 23938, 24020, 24057, 24164, 24176, 24213, 24333, 24349, 24426, 24456, 24457, 24477, 24497, 24502, 24505, 24527, 24534, 24552, 24592, 24654, 24740, 24745, 24753, 24792, 24923, 24954, 24956, 25019, 25054, 25090, 25093, 25176, 25262, 25288, 25305, 25400, 25404, 25424, 25458, 25479, 25549, 25632, 25660, 25683, 25752, 25784, 25802, 25843, 25872, 25892, 25903, 25947, 26001, 26071, 26085, 26157, 26254, 26346, 26497, 26510, 26523, 26594, 26663, 26669, 26676, 26685, 26726, 26777, 26801, 26814, 26836, 26869, 26870, 26884, 26893, 26938, 26968, 26988, 27012, 27061, 27069, 27084, 27107, 27111, 27120, 27191, 27230, 27337, 27363, 27442, 27454, 27455, 27457, 27473, 27526, 27546, 27575, 27658, 27745, 27823, 27862, 27910, 28052, 28066, 28136, 28139, 28146, 28163, 28176, 28198, 28248, 28316, 28328, 28339, 28345, 28356, 28406, 28425, 28452, 28480, 28539, 28547, 28634, 28651, 28682, 28684, 28706, 28707, 28728, 28768, 28796, 28848, 28964, 28972, 28975, 28976, 28985, 28992, 29030, 29034, 29068, 29122, 29140, 29291, 29301, 29312, 29343, 29376, 29381, 29587, 29721, 29734, 29743, 29747, 29748, 29757, 29765, 29895, 29907, 29958, 29964, 29978, 29995, 30135, 30314, 30318, 30329, 30361, 30383, 30418, 30446, 30510, 30545, 30580, 30729, 30779, 30865, 30901, 30922, 30937, 30977, 31000, 31158, 31188, 31257, 31329, 31338, 31357, 31372, 31398, 31418, 31483, 31621, 31632, 31675, 31720]\n"
     ]
    }
   ],
   "source": [
    "unique_label = set(data_y)\n",
    "\n",
    "dic = {k:[] for k in unique_label}\n",
    "for label in unique_label:\n",
    "    for i, v in enumerate(data_y):\n",
    "        if label == v:\n",
    "            dic[label].append(i)\n",
    "print(dic['surprise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "surprise\n",
      "    pos: 26.86%\n",
      "    neg: 73.14%\n",
      "\n",
      "like\n",
      "    pos: 42.44%\n",
      "    neg: 57.56%\n",
      "\n",
      "happiness\n",
      "    pos: 49.74%\n",
      "    neg: 50.26%\n",
      "\n",
      "sadness\n",
      "    pos: 21.58%\n",
      "    neg: 78.42%\n",
      "\n",
      "anger\n",
      "    pos: 16.57%\n",
      "    neg: 83.43%\n",
      "\n",
      "disgust\n",
      "    pos: 22.13%\n",
      "    neg: 77.87%\n",
      "\n",
      "fear\n",
      "    pos: 19.81%\n",
      "    neg: 80.19%\n"
     ]
    }
   ],
   "source": [
    "dic_pos = {k:0. for k in unique_label}\n",
    "dic_neg = {k:0. for k in unique_label}\n",
    "for label in unique_label:\n",
    "    idx = dic[label]\n",
    "    temp_lenght = len(idx)\n",
    "    temp_p = result[idx]\n",
    "    temp_i = np.where(temp_p >= 0.5)[0].shape[0]\n",
    "    pos_rate = temp_i / temp_lenght\n",
    "    dic_pos[label] = pos_rate\n",
    "    dic_neg[label] = 1 - pos_rate\n",
    "\n",
    "for (k1, v1), (k2, v2) in zip(dic_pos.items(), dic_neg.items()):\n",
    "    print()\n",
    "    v1 = np.round(v1 * 100, 2)\n",
    "    v2 = np.round(v2 * 100, 2)\n",
    "    print('{}\\n    pos: {}%\\n    neg: {}%'.format(k1, v1, v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
