{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“lab4-RNN-LM.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOvZsLzoqxrX",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Networks (RNNs) for Classification and Language Modelling  \n",
        "\n",
        "In this notebook we are going to build state-of-the art RNN models for text classification (sentiment analysis) and Language Modelling. We will look into the details of building these models with RNNs and how the performance of those models could be measured efficiently. \n",
        "\n",
        "**NOTE:** Training on the whole corpora in this lab is time consuming on CPU. Make sure that you switch to a GPU runtime in Colab\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWq1iaABCeuN",
        "colab_type": "text"
      },
      "source": [
        "## RNNs Recap\n",
        "\n",
        "RNNs are designed to make use of sequential data, when the current step has some kind of relation with the previous steps. This makes them ideal for applications with a time component (audio, time-series data) and natural language. RNNs are networks for which value of a unit depends on its own previous output as input.\n",
        "\n",
        "An input vector representing the current input element $x_t$ is multiplied by a weight matrix $W$ and then passed through an activation function to compute an activation value for a layer of hidden units. This value is, in turn, used to calculate the output, $y_t$. \n",
        "\n",
        "$h_t = g(Uh_{t-1}+Wx_t)$\n",
        "\n",
        "$y_t = a(Vh_t)$\n",
        "\n",
        "The hidden layer from the previous time step $h_{t-1}$ provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. $U$ determines how the network should make use of past context. RNNs do not impose any limit on this prior context. The context includes information dating back to the beginning of the sequence. Three sets of weights are updated at each timestep: $W$, $U$ and $V$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwyGYr4uCiKe",
        "colab_type": "text"
      },
      "source": [
        "## RNNs for Text Classification Task\n",
        "\n",
        "We will again build a model for sentiment analysis. But now we aim to classify the input text into 3 classes: positive, negative or neutral class. For example, a positive sentence \"I loved the movie\", a negative \"I hated the movie\" and a neutral \"The movie was about Australia\". Usually this is the last RNN hidden state is the one that summarises the whole language sequence.\n",
        "\n",
        "**Q: Why RNNs better than FFNNs?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnAP8gUTHOIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "# This time we will work with a dataset from the torchtext package consists of data processing utilities and popular datasets for NLP\n",
        "from torchtext import datasets\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import math\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# We fix the seeds to get consistent results for each training.\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Helper function to print time between epochs\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7MP6XaihZbj",
        "colab_type": "text"
      },
      "source": [
        "We will experiment with a widely used Stanford Treebank dataset and will predict sentiment of movie reviews. Our data will be classified in three labels: positive, negative and neutral. We take the standard split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpdGmIDlHkpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a tokenizer\n",
        "tokenize = lambda s : nltk.word_tokenize(s)\n",
        "\n",
        "# With TorchText Field we define how our data will be processed\n",
        "\n",
        "TEXT = data.Field(tokenize=tokenize, lower=True)\n",
        "LABEL = data.LabelField(dtype = torch.long)\n",
        "\n",
        "train_data, valid_data, test_data = datasets.SST.splits(\n",
        "            TEXT, LABEL)\n",
        "\n",
        "# Print stat over the data\n",
        "\n",
        "print('train.fields:', train_data.fields)\n",
        "print('len(train):', len(train_data))\n",
        "print('vars(train[0]):', vars(train_data[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWQBmJPCkEAs",
        "colab_type": "text"
      },
      "source": [
        "Print out the first two tokens in the vocabulary. \n",
        "\n",
        "**Q: Why do we map unknown tokens to a special UNK token? Do you think the network will learn a useful embedding for that? If not, how can you let the network to learn an embedding for it?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqdYkiNMd5XX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# here we can use pre-trained embeddings, though too heavy for this environment\n",
        "#TEXT.build_vocab(train_data, vectors=\"glove.6B.50d\")\n",
        "TEXT.build_vocab(train_data)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print('Text Vocabulary Length', len(TEXT.vocab))\n",
        "print (\"Label Vocabulary Length: \", len(LABEL.vocab))\n",
        "\n",
        "#We can display the most common words in the vocabulary and their frequencies\n",
        "\n",
        "print(TEXT.vocab.freqs.most_common(20))\n",
        "\n",
        "#We can also see the vocabulary directly using the stoi (string to int)\n",
        "\n",
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OtsC7oCfxi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# place the tensors on the GPU if available\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# BucketIterator is an iterator that will return a batch of examples of similar lengths, minimizing the amount of padding per example.\n",
        "# Padding refers to fixing the length of inputs (adding a reserved token a certain amount of times to match certain length), usually to the max length within a batch. For example:\n",
        "# i         like  this  movie <pad>\n",
        "# the       movie is    very  good\n",
        "# excellent !     <pad> <pad> <pad>\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "            (train_data, valid_data, test_data), \n",
        "            batch_size = BATCH_SIZE,\n",
        "            sort_within_batch = True,\n",
        "            device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkv98GdXf7pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_iterator)\n",
        "\n",
        "for batch in train_iterator:\n",
        "    demo_batch = batch\n",
        "    break\n",
        "    \n",
        "print(demo_batch)\n",
        "\n",
        "print()\n",
        "\n",
        "# Note that demo_batch.text has a shape of [sentence length x batch size]\n",
        "print(\"Demo batch `text` shape:\", demo_batch.text.shape)\n",
        "# We can simply reshape this into the more familiar [batch size x sentence length]\n",
        "print(\"Demo batch `text` transpose shape:\", demo_batch.text.T.shape)\n",
        "print(\"Demo batch `text` sample: \\n\", demo_batch.text.T[:3, :])\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Demo batch `label` shape:\", demo_batch.label.shape)\n",
        "# shape(demo_batch.label.shape) = [batch size]\n",
        "print(\"Demo batch `label` sample:\", demo_batch.label[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q0xSrBEo2mX",
        "colab_type": "text"
      },
      "source": [
        "## Building the RNN\n",
        "\n",
        "Our RNN class is a sub-class of `nn.Module`. Within the `__init__` method, we define the layers of the module:\n",
        "\n",
        "- Our first layer is an embedding layer (look-up layer). This layer could be initialized with pre-trained embeddings or trained together with other layers.\n",
        " \n",
        "- The next layer is an RNN layer.\n",
        "\n",
        "- Finally, the last linear layer is the output layer for the classification task. This layer receives the last hidden state from the RNN and outputs logits of the output dimensionality.\n",
        "\n",
        "**Q. Fill in the gaps in the code below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4s9rDovrySZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
        "                 bidirectional, dropout, pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # Here, we initialize our model with pre-trained embeddings (50D pre-trained GloVe embeddings in our case).\n",
        "        # This layer will fine-tune these embeddings, specific to this model/dataset.\n",
        "        #self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors)\n",
        "\n",
        "        # We can also train the embeddings from scratch:\n",
        "        self.embedding =  \n",
        "\n",
        "        \n",
        "        # An RNN layer. We specify that the batch dimension goes first\n",
        "        # We have a bidirectional flag which indicates whether the model is unidirectional or bidirectional\n",
        "        # RNNs can be stacked - i.e. have multiple layers. Here, we will only look at the 1 layer case.\n",
        "        self.rnn = nn.RNN(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=bidirectional,\n",
        "                          num_layers=1)\n",
        "\n",
        "          # The linear layer takes the final hidden state and feeds it through a fully connected layer.\n",
        "          # The dimensionality of the output is equal to the output class count.\n",
        "          # For classification in a bidirectional RNN we concatenate:\n",
        "            #  - The last hidden state from the forward RNN (obtained from final word of the sentence)\n",
        "            #  - The last hidden state from the backward RNN (obtained from the first word of the sentence)\n",
        "          # Due to the concatenation, our hidden size is doubled.\n",
        "        \n",
        "        if self.bidirectional:\n",
        "            linear_hidden_in = hidden_dim * 2\n",
        "        else:\n",
        "            linear_hidden_in = hidden_dim\n",
        "\n",
        "        # The classification (linear) layer\n",
        "        self.fc = nn.Linear(linear_hidden_in, output_dim)\n",
        "        \n",
        "\n",
        "        # We apply dropout technique that sets a random set of activations of a layer to zero.\n",
        "        # This prevents the network from learning to rely on specific weights and helps to prevent overfitting. \n",
        "        # Note that the dropout layer is only used during training, and not during test time.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, text):\n",
        "\n",
        "        # ACRONYMS:\n",
        "          # B = Batch size\n",
        "          # T = Max sentence length\n",
        "          # E = Embedding dimension\n",
        "          # D = Hidden dimension\n",
        "          # O = Output dimension\n",
        "\n",
        "        # shape(text) = [B, T]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        # shape(embedded) = [B, T, E]\n",
        "        \n",
        "        # An RNN in PyTorch returns two values:\n",
        "        # (1) All hidden states of the last RNN layer\n",
        "        # (2) Hidden state of the last timestep for every layer\n",
        "        # Note: we are only using 1 layer\n",
        "        all_hidden, last_hidden = self.rnn(embedded)\n",
        "        # shape(all_hidden) = [B, T, D*num_directions]\n",
        "        # shape(last_hidden) = [num_layers*num_directions, B, D].  num_layers = 1\n",
        "        # NOTE. If we were to NOT use the `batch_first` flag, shape of all_hidden would be [T, B, D*num_directions]\n",
        "        \n",
        "        if self.bidirectional:\n",
        "            # Concat the final forward (hidden[0,:,:]) and backward (hidden[1,:,:]) hidden layers\n",
        "            last_hidden = torch.cat((last_hidden[0, :, :], last_hidden[1, :, :]), dim=-1)\n",
        "            # shape(last_hidden) = [B, D*2]\n",
        "\n",
        "        else:\n",
        "            last_hidden = last_hidden.squeeze(0)\n",
        "            # shape(last_hidden) = [B, D]\n",
        "\n",
        "        # Our predictions.\n",
        "        logits = self.fc(self.dropout(last_hidden))\n",
        "        # shape(logits) = [B, O]\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr4QHFDCzQEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 50\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = len(LABEL.vocab)\n",
        "BIDIRECTIONAL = False\n",
        "DROPOUT = 0.3\n",
        "# get our pad token index from the vocabulary\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONiA90vwzTCC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Let's initialise the RNN now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HiKN9dqzXzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNN(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            BIDIRECTIONAL, \n",
        "            DROPOUT, \n",
        "            PAD_IDX).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCbzh45CtAjk",
        "colab_type": "text"
      },
      "source": [
        "**Q. Fill in the gaps in the code below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YlLlXNl0DTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    returns accuracy per batch\n",
        "    \"\"\"\n",
        "\n",
        "    class_preds =  \n",
        "    correct = (class_preds == y).float() # convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKXbQyXitaeo",
        "colab_type": "text"
      },
      "source": [
        "**Q. Fill in the gaps in the code below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCdQ2XfV0E_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "def train(model, train_iterator, valid_iterator, optimizer, criterion, N_EPOCHS=10):\n",
        "    optimizer = optimizer\n",
        "    criterion = criterion\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    for epoch in range(N_EPOCHS):\n",
        "    \n",
        "        start_time = time.time()\n",
        "\n",
        "        # To ensure the dropout is \"turned on\" while training\n",
        "        # (good practice to include in your projects even if it is not used)\n",
        "        model.train()\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "    \n",
        "        # `batch` is a tuple of Tensors: (TEXT, LABEL)\n",
        "        for batch in train_iterator:\n",
        "                        \n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            text = batch.text.to(device)\n",
        "            labels = batch.label.to(device)\n",
        "            # shape(text) = [T, B]\n",
        "            # shape(label) = [B]\n",
        "            \n",
        "            # We reshape text to [B, T]. \n",
        "            # This is purely so we can think about the shapes of the Tensors more consistently\n",
        "            text = text.T\n",
        "            \n",
        "            predictions = model(text)\n",
        "            \n",
        "            # compute the loss\n",
        "            loss =  \n",
        "            print(loss)\n",
        "        \n",
        "            # compute training accuracy\n",
        "            acc = accuracy(predictions, labels)\n",
        "              \n",
        "            # calculate the gradient of each parameter\n",
        "            loss.backward()\n",
        "        \n",
        "            # update the parameters using the gradients and optimizer algorithm \n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            \n",
        "        average_epoch_loss = epoch_loss / len(train_iterator)\n",
        "        average_epoch_acc = epoch_acc / len(train_iterator)\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        \n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "        average_epoch_valid_loss, average_epoch_valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {average_epoch_loss:.3f} | Train Acc: {average_epoch_acc*100:.2f}%')\n",
        "        print(f'\\t Val. Loss: {average_epoch_valid_loss:.3f} |  Val. Acc: {average_epoch_valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceyOzrt509Sp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # Turn on evaluate mode. This de-activates dropout. \n",
        "    model.eval()\n",
        "\n",
        "    # We do not compute gradients within this block, i.e. no training\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text.to(device)\n",
        "            labels = batch.label.to(device)\n",
        "\n",
        "            text = text.T\n",
        "            \n",
        "            predictions = model(text)\n",
        "            loss = criterion(predictions, labels)\n",
        "            acc = accuracy(predictions, labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7FgoAiH1Ngx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "# we use the cross-entropy loss\n",
        "# note that by default losses are averaged over the minibatch\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RneZaqhX_k9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0USTKJzoBI1V",
        "colab_type": "text"
      },
      "source": [
        "## BiDirectional RNNs\n",
        "\n",
        "In problems where all timesteps of the input sequence are available, bidirectional RNNs train two instead of one RNNs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. Outputs for the same sequence step are then usually concatenated. This can provide additional useful context to the model.\n",
        "\n",
        "**Q: Why is a bi-directional RNN is better than single-direction ?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2dLzrb_D0bI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BIDIRECTIONAL = True\n",
        "\n",
        "bidirectional_model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, BIDIRECTIONAL, DROPOUT, PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7SpL8xrGVgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(bidirectional_model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train(bidirectional_model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j4U7ARRLHIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = evaluate(bidirectional_model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3hmLAYOoBD1",
        "colab_type": "text"
      },
      "source": [
        "# Language Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6utgj6xooD36",
        "colab_type": "text"
      },
      "source": [
        "## N-gram language models\n",
        "\n",
        "A language model (LM) assigns a probability to each possible next word, given a history of previous words *i.e.* the context:\n",
        "\n",
        "  $P(w_n|w_1, w_2, \\dots, w_{n-1})=P(w_n|w_1^{n-1})$\n",
        "\n",
        "Since calculating the probability of the whole sentence is not feasible, the **Markov assumption** is introduced: each word depends only on the previous $C$ words, where $C=n-1$ for $n$-gram language models. With this assumption, the above probability is approximated as follows:\n",
        "\n",
        "  - `1-gram ` $\\approx P(w_n)$\n",
        "  - `2-gram ` $\\approx P(w_n|w_{n-1})$\n",
        "  - `3-gram ` $\\approx P(w_n|w_{n-2}, w_{n-1})$\n",
        "  - `N-gram ` $\\approx P(w_n|w_{n-N+1}, \\dots, w_{n-2}, w_{n-1})$\n",
        "\n",
        "The n-gram probabilities are then calculated by normalising the count of that n-gram by the count of the context/prefix:\n",
        "\n",
        "  $P(w_n|w_{n-N+1}, \\dots, w_{n-2}, w_{n-1})=\\large\\frac{C(w_{n-N+1}, \\dots, w_{n-1}, w_{n})}{C(w_{n-N+1}, \\dots, w_{n-1})}$\n",
        "\n",
        "\n",
        "Smoothing is used to deal with the sparsity problem in the N-Gram LM. With smoothing the probability mass  shifted towards the less frequent words.\n",
        "\n",
        "For example, an add-1 smoothing for a bigram model:\n",
        "\n",
        "$P_{add-1}(w_n|w_{n-1})=\\frac{C(w_{n-1},w_n)+1}{C(w_{n-1})+V}$\n",
        "\n",
        "**Q: What is the disadvantage of smoothing?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w66qJPsIxdbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import operator\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "class NGramLM():\n",
        "    def __init__(self, N):\n",
        "        self.N = N\n",
        "        self.vocab = set()\n",
        "        self.data = []\n",
        "        self.prob = {}\n",
        "        self.count = defaultdict(Counter)\n",
        "    \n",
        "    # For N = 1, the probability is stored in a dict   P = prob[next_word]\n",
        "    # For N > 1, the probability is in a nested dict   P = prob[context][next_word]\n",
        "    def train(self, vocab, data, smoothing_k=0):\n",
        "        self.vocab = vocab\n",
        "        self.data = data\n",
        "        self.smoothing_k = smoothing_k\n",
        "\n",
        "        if self.N == 1:\n",
        "            self.counts = Counter(flatten(data))\n",
        "            self.prob = self.get_prob(self.counts)\n",
        "        else:\n",
        "            self.vocab.add('<s>')\n",
        "            counts = self.count_ngram()\n",
        "            \n",
        "            self.prob = {}\n",
        "            for context, counter in counts.items():\n",
        "                self.prob[context] = self.get_prob(counter)\n",
        "    \n",
        "    def count_ngram(self):\n",
        "        counts = defaultdict(Counter)\n",
        "        for sentence in self.data:\n",
        "            sentence = (self.N - 1) * ['<s>'] + sentence \n",
        "            for i in range(len(sentence)-self.N+1):\n",
        "                context = sentence[i:i+self.N-1]\n",
        "                context = \" \".join(context)\n",
        "                word = sentence[i+self.N-1]\n",
        "                counts[context][word] += 1\n",
        "\n",
        "        self.counts = counts\n",
        "        return counts\n",
        "        \n",
        "    # normalize counts into probability(considering smoothing)\n",
        "    def get_prob(self, counter):\n",
        "        total = float(sum(counter.values()))\n",
        "        k = self.smoothing_k\n",
        "        \n",
        "        prob = {}\n",
        "        for word, count in counter.items():\n",
        "            prob[word] = (count + k) / (total + len(self.vocab) * k)\n",
        "        return prob\n",
        "        \n",
        "    def get_ngram_logprob(self, word, seq_len, context=\"\"):\n",
        "        if self.N == 1 and word in self.prob.keys():\n",
        "            return math.log(self.prob[word]) / seq_len\n",
        "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
        "            return math.log(self.prob[context][word]) / seq_len\n",
        "        else:\n",
        "            # assign a small probability to the unseen ngram\n",
        "            # to avoid log of zero and to penalise unseen word or context\n",
        "            return math.log(1/len(self.vocab)) / seq_len\n",
        "        \n",
        "    def get_ngram_prob(self, word, context=\"\"):\n",
        "        if self.N == 1 and word in self.prob.keys():\n",
        "            return self.prob[word]\n",
        "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
        "            return self.prob[context][word]\n",
        "        elif word in self.vocab and self.smoothing_k > 0:\n",
        "            # probability assigned by smoothing\n",
        "            return self.smoothing_k / (sum(self.counts[context].values()) + self.smoothing_k*len(self.vocab))\n",
        "        else:\n",
        "            # unseen word or context\n",
        "            return 0\n",
        "    \n",
        "    def _is_unseen_ngram(self, context, word):\n",
        "        if context not in self.prob.keys() or word not in self.prob[context].keys():\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    # generate the most probable k words\n",
        "    def generate_next(self, context, k):\n",
        "        contex = (self.N-1) * '<s>' + context\n",
        "        context = context.split()\n",
        "        ngram_context_list = context[-self.N+1:]\n",
        "        ngram_context = \" \".join(ngram_context_list)\n",
        "        \n",
        "        if ngram_context in self.prob.keys():\n",
        "            candidates = self.prob[ngram_context]\n",
        "            most_probable_words = sorted(candidates.items(), key=lambda kv: kv[1], reverse=True)\n",
        "            for i in range(min(k, len(most_probable_words))):\n",
        "                print(\" \".join(context)+\" \"+most_probable_words[i][0]+\"\\t P={}\".format(most_probable_words[i][1]))\n",
        "        else:\n",
        "            print(\"Unseen context!\")\n",
        "            \n",
        "    # generate the next n words with greedy search\n",
        "    def generate_next_n(self, context, n):\n",
        "        contex = (self.N-1) * '<s>' + context\n",
        "        context = context.split()\n",
        "        ngram_context_list = context[-self.N+1:]\n",
        "        ngram_context = \" \".join(ngram_context_list)\n",
        "        \n",
        "        for i in range(n):\n",
        "            try:\n",
        "                candidates = self.prob[ngram_context]\n",
        "                most_likely_next = max(candidates.items(), key=operator.itemgetter(1))[0]\n",
        "                context += [most_likely_next]\n",
        "                ngram_context_list = ngram_context_list[1:] + [most_likely_next]\n",
        "                ngram_context = \" \".join(ngram_context_list)\n",
        "            except:\n",
        "                break\n",
        "        print(\" \".join(context))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hupESvtA0vmS",
        "colab_type": "text"
      },
      "source": [
        "Let's train a bigram Language Model on the toy dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuVfj9YA0pLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\"I like ice cream\",\n",
        "         \"I like chocolate\",\n",
        "         \"I hate beans\"]\n",
        "tokens = [l.strip().lower().split() + ['</s>'] for l in corpus if l.strip()]\n",
        "vocab = set(flatten(tokens))\n",
        "print(tokens)\n",
        "print(vocab)\n",
        "\n",
        "def print_probability(lm):\n",
        "    for context in lm.vocab:\n",
        "        for word in lm.vocab:\n",
        "            prob = lm.get_ngram_prob(word, context)\n",
        "            print(\"P({}\\t|{}) = {}\".format(word, context, prob))\n",
        "        print(\"--------------------------\")\n",
        "\n",
        "lm = NGramLM(2)\n",
        "\n",
        "# here we do not use smoothing\n",
        "\n",
        "lm.train(vocab, tokens, smoothing_k=0)\n",
        "\n",
        "print_probability(lm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JEtXjakIrvUH"
      },
      "source": [
        "# Neural Language Modelling\n",
        "\n",
        "We will be using the WikiText-2 corpus, which is a popular LM dataset. The WikiText language modeling dataset is a collection of texts extracted from articles on Wikipedia.\n",
        "It contains about 2 million words. \n",
        "\n",
        "**Q: What is the difference between word embeddings and language modelling?**\n",
        "\n",
        "\n",
        "**Q: How is the output of the network is computed ?**\n",
        "\n",
        "\n",
        "**Q: What is the training loss for LM tasks ?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YznlehLh-73h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the corpus\n",
        "%%bash\n",
        "URL=\"https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2\"\n",
        "\n",
        "for split in \"train\" \"valid\" \"test\"; do\n",
        "  if [ ! -f \"${split}.txt\" ]; then\n",
        "    echo \"Downloading ${split}.txt\"\n",
        "    wget -q \"${URL}/${split}.txt\"\n",
        "    # Remove empty lines\n",
        "    sed -i '/^ *$/d' \"${split}.txt\"\n",
        "    # Remove article titles starting with = and ending with =\n",
        "    sed -i '/^ *= .* = $/d' \"${split}\".txt\n",
        "  fi\n",
        "done\n",
        "\n",
        "# Prepare smaller version for fast training neural LMs\n",
        "head -n 5000 < train.txt > train_small.txt\n",
        "\n",
        "# Print the first 10 lines with line numbers\n",
        "cat -n train.txt | head -n10\n",
        "echo\n",
        "\n",
        "# Print some statistics\n",
        "echo -e \"\\n   Line,   word,   character counts\"\n",
        "wc *.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaJf6U31L1EL",
        "colab_type": "text"
      },
      "source": [
        "The below Vocabulary class encapsulates the *word-to-idx* and *idx-to-word* mapping that you should now be familiar with from the previous lab sessions. Read it to understand how the vocabulary is constructed from a plain text file, within the `build_from_file()` method. Special <.> markers are also included in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4xq2Xy3LkJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary(object):\n",
        "  \"\"\"Data structure representing the vocabulary of a corpus.\"\"\"\n",
        "  def __init__(self):\n",
        "    # Mapping from tokens to integers\n",
        "    self._word2idx = {}\n",
        "\n",
        "    # Reverse-mapping from integers to tokens\n",
        "    self.idx2word = []\n",
        "\n",
        "    # 0-padding token\n",
        "    self.add_word('<pad>')\n",
        "    # Unknown words\n",
        "    self.add_word('<unk>')\n",
        "\n",
        "    self._unk_idx = self._word2idx['<unk>']\n",
        "\n",
        "  def word2idx(self, word):\n",
        "    \"\"\"Returns the integer ID of the word or <unk> if not found.\"\"\"\n",
        "    return self._word2idx.get(word, self._unk_idx)\n",
        "\n",
        "  def add_word(self, word):\n",
        "    \"\"\"Adds the `word` into the vocabulary.\"\"\"\n",
        "    if word not in self._word2idx:\n",
        "      self.idx2word.append(word)\n",
        "      self._word2idx[word] = len(self.idx2word) - 1\n",
        "\n",
        "  def build_from_file(self, fname):\n",
        "    \"\"\"Builds a vocabulary from a given corpus file.\"\"\"\n",
        "    with open(fname) as f:\n",
        "      for line in f:\n",
        "        words = line.strip().split()\n",
        "        for word in words:\n",
        "          self.add_word(word)\n",
        "\n",
        "  def convert_idxs_to_words(self, idxs):\n",
        "    \"\"\"Converts a list of indices to words.\"\"\"\n",
        "    return ' '.join(self.idx2word[idx] for idx in idxs)\n",
        "\n",
        "  def convert_words_to_idxs(self, words):\n",
        "    \"\"\"Converts a list of words to a list of indices.\"\"\"\n",
        "    return [self.word2idx(w) for w in words]\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Returns the size of the vocabulary.\"\"\"\n",
        "    return len(self.idx2word)\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return \"Vocabulary with {} items\".format(self.__len__())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW6OM2_TMWg7",
        "colab_type": "text"
      },
      "source": [
        "Let's construct the vocabulary for the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uXcXImVMYIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocabulary()\n",
        "vocab.build_from_file('train_small.txt')\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojvpYNTQMc6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readable_size(n):\n",
        "  \"\"\"Returns a readable size string for model parameters count.\"\"\"\n",
        "  sizes = ['K', 'M', 'G']\n",
        "  fmt = ''\n",
        "  size = n\n",
        "  for i, s in enumerate(sizes):\n",
        "    nn = n / (1000 ** (i + 1))\n",
        "    if nn >= 1:\n",
        "      size = nn\n",
        "      fmt = sizes[i]\n",
        "    else:\n",
        "      break\n",
        "  return '%.2f%s' % (size, fmt)\n",
        "\n",
        "def corpus_to_tensor(_vocab, filename):\n",
        "  # Final token indices\n",
        "  idxs = []\n",
        "  \n",
        "  with open(filename) as data:\n",
        "    for line in tqdm(data, ncols=80, unit=' line', desc=f'Reading {filename} '):\n",
        "      line = line.strip()\n",
        "      # Skip empty lines if any\n",
        "      if line:\n",
        "        # Split from whitespace and add sentence markers\n",
        "        idxs.extend(_vocab.convert_words_to_idxs(line.split()))\n",
        "  return torch.LongTensor(idxs)\n",
        "\n",
        "train_small = corpus_to_tensor(vocab, 'train_small.txt')\n",
        "\n",
        "valid = corpus_to_tensor(vocab, 'valid.txt')\n",
        "test = corpus_to_tensor(vocab, 'test.txt')\n",
        "print('\\n')\n",
        "\n",
        "print(f'Small training size in tokens: {readable_size(len(train_small))}')\n",
        "print(f'Validation size in tokens: {readable_size(len(valid))}')\n",
        "print(f'Test size in tokens: {readable_size(len(test))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b49ea9Fo7aCe",
        "colab_type": "text"
      },
      "source": [
        "## Feed-forward Language Models (FFLM)\n",
        "\n",
        "FFLMs are similar to $n$-gram language models in the sense that the choice of $n$ is a hyperparameter for the network architecture. A basic FFLM constructs a $C=n\\mathrm{-1}$ length context window before the word to be predicted. If the word embedding size is $E$, the feature vector for the context window becomes a vector of size $E\\times C$, resulting from the concatenation of individual word embeddings of context words. Hence, the choice of $C$ for FFLMs, affects the number of final learnable parameters in the network.\n",
        "\n",
        "**Q. Fill in the gaps in the code below.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwwaQ2TS7Ygq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FFLM(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim, hid_dim, context_size, dropout=0.5):\n",
        "    # Call parent's __init__ first\n",
        "    super(FFLM, self).__init__()\n",
        "    \n",
        "    # Store arguments\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.context_size = context_size\n",
        "\n",
        "    # Create the loss, don't sum or average, we'll take care of it\n",
        "    # in the training loop for logging purposes\n",
        "    self.loss = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    # Create the non-linearity\n",
        "    self.nonlin = torch.nn.Tanh()\n",
        "\n",
        "    # Dropout regularizer\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "    # Compute the dimension of the context vector\n",
        "    self.context_dim =  \n",
        "    \n",
        "    # Create the embedding layer (i.e. lookup table tokens->vectors)\n",
        "    self.emb = nn.Embedding(\n",
        "        num_embeddings=self.vocab_size, embedding_dim=self.emb_dim,\n",
        "        padding_idx=0)\n",
        " \n",
        "    # This cuts the number of parameters a bit\n",
        "    self.ff_ctx = nn.Linear(self.context_dim, self.hid_dim)\n",
        "\n",
        "    # Output layer mapping from the output of `ff_ctx` to vocabulary size\n",
        "    self.out = nn.Linear(self.hid_dim, self.vocab_size)\n",
        "\n",
        "    # Purely for informational purposes: compute # of total params\n",
        "    self.n_params = 0\n",
        "    for param in self.parameters():\n",
        "        self.n_params += np.cumprod(param.data.size())[-1]\n",
        "    self.n_params = readable_size(self.n_params)\n",
        "      \n",
        "  def forward(self, x, y):\n",
        "    \"\"\"Forward-pass of the module.\"\"\"\n",
        "    # Shape of x is (batch_size, context_size)\n",
        "\n",
        "    # Get the embeddings for the token indices in `x`\n",
        "    embs = self.emb(x)\n",
        "\n",
        "    # Concatenate the embeddings to form the context vector\n",
        "    ctx = embs.view(embs.shape[0], -1)\n",
        "\n",
        "    # apply ff_ctx -> non-lin -> dropout -> output layer\n",
        "    ctx = self.drop(self.nonlin(self.ff_ctx(ctx)))\n",
        "    logits = self.out(ctx)\n",
        "    return self.loss(logits, y)\n",
        "\n",
        "  def get_batches(self, data_tensor, batch_size=64):\n",
        "    \"\"\"Returns a tensor of size (n_batches, batch_size, context_size + 1).\"\"\"\n",
        "    # Split data into rows of n-grams followed by the (n+1)th true label\n",
        "    x_y = data_tensor.unfold(0, self.context_size + 1, step=1)\n",
        "\n",
        "    # Get the number of training n-grams\n",
        "    n_samples = x_y.size()[0]\n",
        "\n",
        "    # hack: discard the last uneven batch for simplicity\n",
        "    n_batches = n_samples // batch_size\n",
        "    n_samples = n_batches * batch_size\n",
        "    # Split nicely into batches, i.e. (n_batches, batch_size, context_size + 1)\n",
        "    # The final element in each row is the ID of the true label to predict\n",
        "    x_y = x_y[:n_samples].view(n_batches, batch_size, -1)\n",
        "\n",
        "    # A particular batch for context_size=2 will now look like below in\n",
        "    # word format. Last element for every array is the next token to be predicted\n",
        "    #\n",
        "    # [[<s>, cat, sat],\n",
        "    #  [cat, sat, on],\n",
        "    #  [sat, on,  the],\n",
        "    #  [on,  the, mat],\n",
        "    #   ....\n",
        "    return x_y\n",
        "\n",
        "  def train_model(self, optim, train_tensor, valid_tensor, test_tensor, n_epochs=5,\n",
        "                 batch_size=64, shuffle=False):\n",
        "    \"\"\"Trains the model.\"\"\"\n",
        "    # Get batches for the training data\n",
        "    batches = self.get_batches(train_tensor, batch_size)\n",
        "    \n",
        "    print(f'Will do {batches.size(0)} batches for an epoch.')\n",
        "\n",
        "    for eidx in range(1, n_epochs + 1):\n",
        "      start_time = time.time()\n",
        "      epoch_loss = 0\n",
        "      epoch_items = 0\n",
        "\n",
        "      # Enable training mode\n",
        "      self.train()\n",
        "\n",
        "      # Shuffle the batch order or not\n",
        "      if shuffle:\n",
        "        batch_order = torch.randperm(batches.size(0))\n",
        "      else:\n",
        "        batch_order = torch.arange(batches.size(0))\n",
        "\n",
        "      # Start training\n",
        "      for iter_count, idx in enumerate(batch_order):\n",
        "        batch = batches[idx].to(device)\n",
        "\n",
        "        # split into inputs `x` and labels `y`\n",
        "        x, y = batch[:, :self.context_size], batch[:, -1]\n",
        "\n",
        "        # Clear the gradients\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # loss will be a vector of size (batch_size, ) with losses per every sample\n",
        "        loss = self.forward(x, y)\n",
        "\n",
        "        # Backprop the average loss and update parameters\n",
        "        loss.mean().backward()\n",
        "        optim.step()\n",
        "\n",
        "        # sum the loss for reporting, along with the denominator\n",
        "        epoch_loss += loss.detach().sum()\n",
        "        epoch_items += loss.numel()\n",
        "\n",
        "        if iter_count % 1000 == 0:\n",
        "          # Print progress\n",
        "          loss_per_token = epoch_loss / epoch_items\n",
        "          print(f'[Epoch {eidx:<3}] loss: {loss_per_token:6.2f}')\n",
        "\n",
        "      time_spent = time.time() - start_time\n",
        "\n",
        "      print(f'\\n[Epoch {eidx:<3}] ended with train_loss: {loss_per_token:6.2f}')\n",
        "      # Evaluate on valid set\n",
        "      valid_loss = self.evaluate(test_set=valid_tensor)\n",
        "      print(f'[Epoch {eidx:<3}] ended with valid_loss: {valid_loss:6.2f}')\n",
        "      print(f'[Epoch {eidx:<3}] completed in {time_spent:.2f} seconds\\n')\n",
        "\n",
        "    # Evaluate the final model on test set\n",
        "    test_loss = self.evaluate(test_set=test_tensor)\n",
        "    print(f' ---> Final test set performance: {test_loss:6.2f}')\n",
        "\n",
        "  def evaluate(self, test_set, batch_size=32):\n",
        "    \"\"\"Evaluates and computes perplexity for the given test set.\"\"\"\n",
        "    loss = 0\n",
        "\n",
        "    # Get the batches\n",
        "    batches = self.get_batches(test_set, batch_size)\n",
        "\n",
        "    # Eval mode\n",
        "    self.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in batches:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # split into inputs `x` and labels `y`\n",
        "        x, y = batch[:, :self.context_size], batch[:, -1]\n",
        "\n",
        "        # loss will be a vector of size (batch_size, ) with losses per every sample\n",
        "        # sum the loss for reporting, along with the denominator\n",
        "        loss += self.forward(x, y).sum()\n",
        "    \n",
        "    # Normalize by the number of tokens in the test set\n",
        "    loss /= batches.size()[:2].numel()\n",
        "\n",
        "    # Switch back to training mode\n",
        "    self.train()\n",
        "\n",
        "    # return the loss\n",
        "    return loss\n",
        "\n",
        "  def __repr__(self):\n",
        "    \"\"\"String representation for pretty-printing.\"\"\"\n",
        "    s = super(FFLM, self).__repr__()\n",
        "    return f\"{s}\\n# of parameters: {self.n_params}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbEHST9I8rGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fflm_model = FFLM(\n",
        "    len(vocab),       # vocabulary size\n",
        "    emb_dim=128,      # word embedding dim\n",
        "    hid_dim=128,      # hidden layer dim\n",
        "    context_size=2,   # C = (N-1) if you think in n-gram LM terminology\n",
        "    dropout=0.4,      # dropout probability\n",
        ")\n",
        "\n",
        "# move to device\n",
        "fflm_model.to(device)\n",
        "\n",
        "# Initial learning rate for the optimizer\n",
        "FFLM_INIT_LR = 0.001\n",
        "\n",
        "# Create the optimizer\n",
        "fflm_optimizer = torch.optim.Adam(fflm_model.parameters(), lr=FFLM_INIT_LR)\n",
        "print(fflm_model)\n",
        "\n",
        "fflm_model.train_model(fflm_optimizer, train_small, valid, test, n_epochs=5, batch_size=64, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW4dXymxiAn7",
        "colab_type": "text"
      },
      "source": [
        "## Recurrent Language Models (RNNLM)\n",
        "\n",
        "Now we switch to more complex RNN LMs which have access to large context windows.\n",
        "\n",
        "Here for pre-processing we will use the `torchtext` package with data processing utilities and popular datasets for natural language. We will again work with the WikiText2 dataset.\n",
        "\n",
        "**Q. Why do we need the BOS token ?**\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSWrnr00LA54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenize = lambda s : nltk.word_tokenize(s)\n",
        "# With TorchText Field we define how our data will be processed\n",
        "TEXT = data.Field(tokenize = tokenize, init_token = '<bos>')\n",
        "\n",
        "train_data, valid_data, test_data = datasets.WikiText2.splits(TEXT)\n",
        "\n",
        "# Data stats\n",
        "print('train.fields', train_data.fields)\n",
        "print('len(train)', len(train_data))\n",
        "\n",
        "# Build a vocabulary out of tokens available from the pre-trained embeddings list and the vocabulary of labels\n",
        "TEXT.build_vocab(train_data)\n",
        "print('Text Vocabulary Length', len(TEXT.vocab))\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# place the tensors on the GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f32tFT7TrvUc"
      },
      "source": [
        "Backpropagation Through Time, or BPTT, is the application of the Backpropagation training algorithm to RNNs. BPTT works by unrolling all input timesteps. Each timestep has one input timestep, one copy of the network, and one output. Errors are then calculated and accumulated for each timestep. If input sequences are comprised of thousands of timesteps, then this will be the number of derivatives required for a single update weight update. This can cause weights to vanish or explode. Truncated Backpropagation Through Time, or TBPTT, is a modified version of the BPTT where the BPTT update is performed back for a fixed number of timesteps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBpF7Y-MMDDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenize = lambda s : nltk.word_tokenize(s)\n",
        "# With TorchText Field we define how our data will be processed\n",
        "TEXT = data.Field(tokenize = tokenize, init_token = '<bos>')\n",
        "\n",
        "train_data, valid_data, test_data = datasets.WikiText2.splits(TEXT)\n",
        "\n",
        "# Data stats\n",
        "print('train.fields', train_data.fields)\n",
        "print('len(train)', len(train_data))\n",
        "\n",
        "# Build a vocabulary out of tokens available from the pre-trained embeddings list and the vocabulary of labels\n",
        "TEXT.build_vocab(train_data)\n",
        "print('Text Vocabulary Length', len(TEXT.vocab))\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# place the tensors on the GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaTig0qHxyI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout, pad_idx):\n",
        "            \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # UNIDIRECTIONAL RNN layer: For LM modelling we do not see/have access to the right context\n",
        "        \n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        " \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "       \n",
        "    def forward(self, text, prev_hidden):\n",
        "         \n",
        "        # shape(text) = [B, T]\n",
        "        \n",
        "         \n",
        "        # shape(prev_hidden) = [1, B, D] where 1 = num_layers*num_directions\n",
        "   \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        # shape(embedded) = [B, T, E]\n",
        "        \n",
        "        all_hidden, last_hidden = self.rnn(embedded, prev_hidden)        \n",
        "        # shape(all_hidden) = [B, T, D]\n",
        "        # shape(last_hidden) = [num layers, B, D]\n",
        " \n",
        "        # Take all hidden states to produce an output word per time step\n",
        "        \n",
        "        logits = self.fc(self.dropout(all_hidden))\n",
        "        # shape(logits) = [B, O]\n",
        "            \n",
        "        return logits, last_hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbFtwxU2yRK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = len(TEXT.vocab)\n",
        "DROPOUT = 0.5\n",
        "# get our pad token index from the vocabulary\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "rnn_model = RNN(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            DROPOUT, \n",
        "            PAD_IDX)\n",
        "\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aXkUub7yp6P",
        "colab_type": "text"
      },
      "source": [
        "We need to detach the hidden state or else the model will try to backpropagate to the beginning of the dataset, requiring a lot of memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziJcZXzMzwlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_hidden(hidden):\n",
        "  \"\"\"Wraps hidden states in new Tensors, to declare it not to need gradients. So that the initial hidden state for this batch is constant and doesn’t depend on anything.\"\"\"\n",
        "\n",
        "  if isinstance(hidden, torch.Tensor):\n",
        "    return hidden.detach()\n",
        "  else:\n",
        "    return tuple(save_hidden(v) for v in hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qnQtfeXz78O",
        "colab_type": "text"
      },
      "source": [
        "## Perplexity\n",
        "\n",
        "Language is very difficult to evaluate since there is no single gold truth: one meaning could be expressed in many valid ways. Human evaluation of a language model may involve how a hypothesis satisfies the grammatical and lexical norms of a language. Human evaluation is reliable, but costly and slow. Automatic metrics are a less costly and faster option.\n",
        "\n",
        "For example, **perplexity** can reveal if the model prefers real (=frequently observed) sentences to ‘ungrammatical/gibberish’ (or rarely observed) ones. Remember that entropy is the average number of bits to encode the information contained in a random variable, so the exponentiation of the entropy (perplexity, $e^{H}$) should be the total amount of all possible information, or more precisely, the weighted average number of choices a random variable has. We evaluate our prediction Q by testing against samples drawn from P: $PPL = e^{CrossEntropy}$.\n",
        "\n",
        "We usually measure perplexity on an unseen (test) corpus, generally we compare a range of models using this score. The best LM is the one that generates the lowest perplexity on the test corpus.\n",
        "\n",
        "**Q. Fill in the gaps in the code below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdn32mpY0kcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perplexity(loss_per_token):\n",
        "    return  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKjKOKhO19wi",
        "colab_type": "text"
      },
      "source": [
        "**Q: How are language models trained ?**\n",
        "\n",
        "**Q. Fill in the gaps in the code below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px9V_F6K0o13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_iterator, valid_iterator, optimizer, criterion, N_EPOCHS=10, is_lstm=False, force_stop=False):\n",
        "    \n",
        "    optimizer = optimizer\n",
        "    criterion = criterion\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "    \n",
        "    for epoch in range(N_EPOCHS):\n",
        "    \n",
        "        start_time = time.time()\n",
        "        \n",
        "        model.train()\n",
        "\n",
        "        epoch_loss = 0\n",
        "        epoch_items = 0\n",
        "        \n",
        "        # The `1` is the number of layers * number of directions.\n",
        "        # i.e. we have 1 layer and we are moving in 1 direction\n",
        "        prev_hidden = torch.zeros(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
        "  \n",
        "        \n",
        "        # `batch` is a tuple of Tensors: (TEXT, TARGET)\n",
        "        for i, batch in enumerate(train_iterator):\n",
        "            \n",
        "            if force_stop:\n",
        "                print(\"Currently processing train batch {} of {}\".format(i, len(train_iterator)))\n",
        "                if i % 7 == 0 and i != 0:\n",
        "                    break\n",
        "            \n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            text = batch.text\n",
        "            targets = batch.target\n",
        "            print()\n",
        "            # shape(text) = [T, B]\n",
        "            # shape(target) = [T, B]\n",
        "            \n",
        "            # We reshape text and target to [B, T]. \n",
        "            text = text.T.to(device)\n",
        "            targets = targets.T.to(device)\n",
        "            # shape(text) = [B, T]\n",
        "            # shape(target) = [B, T]\n",
        "                        \n",
        "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "            # Otherwise the model would backpropagate all the way to beginning of the dataset.\n",
        "            prev_hidden = save_hidden(prev_hidden)\n",
        "            \n",
        "            ## run the model\n",
        "            logits, prev_hidden = model(text, prev_hidden)\n",
        "            \n",
        "            # Compute the loss\n",
        "            # We reshape inputs to eliminate batching\n",
        "            \n",
        "            print(\"logits\", logits.view(-1, OUTPUT_DIM).shape)\n",
        "            print(\"targets\", targets.reshape(-1).shape)\n",
        "            \n",
        "            loss = \n",
        "            # backprop the average loss and update parameters\n",
        "            loss.backward()\n",
        "        \n",
        "            # update the parameters using the gradients and optimizer algorithm \n",
        "            ##call the optimizer\n",
        "            optimizer.step()\n",
        "            \n",
        "            \n",
        "            epoch_loss += loss.detach()\n",
        "            epoch_items += loss.numel()\n",
        "        \n",
        "        # We compute loss per token for an epoch\n",
        "        train_loss_per_token = epoch_loss / epoch_items\n",
        "        # We compute perplexity\n",
        "        train_ppl = perplexity(train_loss_per_token)\n",
        "\n",
        "        end_time = time.time()        \n",
        "        \n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "        valid_loss_per_token, valid_ppl = evaluate(model, \n",
        "                                                   valid_iterator, \n",
        "                                                   criterion,\n",
        "                                                   is_lstm=is_lstm,\n",
        "                                                   force_stop=force_stop)\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss_per_token:.3f} | Train Perplexity: {train_ppl:.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss_per_token:.3f} |  Val. Perplexity: {valid_ppl:.3f}')\n",
        "        \n",
        "        if force_stop:\n",
        "            break\n",
        "\n",
        "def evaluate(model, iterator, criterion, is_lstm=False, force_stop=False):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_items = 0\n",
        "    \n",
        "    # we initialise the first hidden state with zeros\n",
        "    ## Initialise the previous hidden states of the RNNs\n",
        "    prev_hidden = torch.zeros(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
        " \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            \n",
        "            if force_stop and i % 3 == 0 and i != 0:\n",
        "                print(\"Currently processing valid batch {} of {}\".format(i, len(train_iterator)))\n",
        "                break\n",
        "\n",
        "            text, target = batch.text, batch.target\n",
        "            text, target = text.T, target.T\n",
        "            logits, prev_hidden = model(text, prev_hidden)\n",
        "\n",
        "            # compute the loss\n",
        "            loss =  \n",
        "\n",
        "            prev_hidden = save_hidden(prev_hidden)\n",
        "\n",
        "            epoch_loss += loss.detach()\n",
        "            epoch_items += loss.numel()\n",
        "\n",
        "        loss_per_token = epoch_loss / epoch_items\n",
        "        ppl = math.exp(loss_per_token)\n",
        "            \n",
        "        \n",
        "    return loss_per_token, ppl\n",
        "\n",
        "optimizer = optim.Adam(rnn_model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train(rnn_model, train_iterator, valid_iterator, optimizer, criterion, force_stop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdJ2xaZjqw5u",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Advanced: Long short-term memory architectures LSTMs vs. RNNs\n",
        "\n",
        "The gradient signal gets smaller and smaller as it backpropagates further. It is caused by the repeated use of the recurrent weight matrix in RNN. Gradient can be viewed as a measure of the effect of the past on the future. If the gradient becomes vanishingly small over longer distances we can not capture the dependency to the past correctly. For example: \"A patient with a rare sarcoma of soft tissue on the left thigh was presented to the hospital yesterday.\" \"was presented\" depends on \"a patient\", but they are separated by 11 words!\n",
        "\n",
        "LSTM has two \"hidden states\": $c_t$  and $h_t$ . You can think of $c_t$  is the \"internal\" hidden state that retains important information for longer timesteps, whereas $h_t$ is the \"external\" hidden state that exposes that information to the outside world.\n",
        "\n",
        "The LSTM does have the ability to remove or add information to the cell state. Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.  An LSTM has three of these gates.\n",
        "\n",
        "Forget gate decides what information we’re going to throw away from the cell state. \n",
        "\n",
        "$f_t = \\sigma(W_{if}x_t + W_{hf}h_{t-1}+b_f)$\n",
        "\n",
        "$\\sigma$ squashes input values between 0 and 1, describing how much of each component should be let through. Zero means \"let nothing through\", while a value of one means \"let everything through\".\n",
        "\n",
        "Input gate decides what new information we are going to store in the cell state. \n",
        "\n",
        "$i_t = \\sigma(W_{ii}x_t + W_{hi}h_{t-1}+b_i)$\n",
        "\n",
        "Next, a tanh layer creates a vector of new candidate values, $g_t$, that could be added to the state. tanh squashes the output values to be between −1 and 1. \n",
        "\n",
        "$g_t = tanh(W_{ig}x_t + W_{hg}h_{t-1}+b_g)$ (this equation equal to vanilla RNN if we remove gates)\n",
        "\n",
        "The next step combines these two to create an update to the state. Pointwise multiplication operation (*) decides on the parts we output.\n",
        "\n",
        "$c_t = f_t * c_{t-1} + i_t * g_t$\n",
        "\n",
        "Finally, the output gate decides how much information goes to the output:\n",
        "\n",
        "$o_t = \\sigma(W_{io}x_t + W_{ho}h_{t-1}+b_o)$\n",
        "\n",
        "$h_t = o_t * tanh(c_t)$\n",
        " \n",
        "**Q: How does this help with the vanishing gradient problem ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_7aCGiAwYET",
        "colab_type": "text"
      },
      "source": [
        "## LSTMs vs. GRUs\n",
        "\n",
        "Gated Recurrent Unit (GRU) combines the forget and input gates into a single \"update gate\" (z). So we have only two gates: update and reset. It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models. Candidate state $g_t$ is able to suppress $h_t$. The final state is a convex combination: of the $g_t$ and $h_{t-1}$ with coefficients of $(1 - z_t)$ and $z_t$ respectively.\n",
        "\n",
        "$r_t = \\sigma(W_{ir}x_t + W_{hr}h_{t-1}+b_r)$\n",
        "\n",
        "$z_t = \\sigma(W_{iz}x_t + W_{hz}h_{t-1}+b_z)$\n",
        "\n",
        "$g_t = tanh(W_{ig}x_t + r_t * (W_{hg}h_{t-1}+b_g))$\n",
        "\n",
        "$h_t = (1 - z_t)* g_t + z_t * h_{t-1}$\n",
        "\n",
        "\n",
        "**Q. Implement an LSTM cell from scratch. Compare its performance to RNN and LSTM from the Pytorch toolkit.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1aZwx5T41dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    \n",
        "    # variant is a flag which is either: \"rnn\", \"lstm\", \"manual_lstm\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout, pad_idx, variant):\n",
        "            \n",
        "        super().__init__()\n",
        "        \n",
        "        self.variant = variant\n",
        "        \n",
        "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors)\n",
        "\n",
        "        # UNIDIRECTIONAL RNN layer: For LM modelling we do not see/have access to the right context\n",
        "        \n",
        "        if variant == \"rnn\":\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        elif variant == \"lstm\":\n",
        "            self.rnn = nn.LSTM(embedding_dim, \n",
        "                               hidden_dim, \n",
        "                               batch_first=True)\n",
        "        elif variant == \"my_lstm\":\n",
        "            self.rnn = My_LSTM(embedding_dim, hidden_dim)\n",
        "        else:\n",
        "            raise ValueError(\"Expected `variant` to be one of 'rnn', 'lstm', or 'my_lstm'\")\n",
        "            \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "       \n",
        "    def forward(self, text, prev_hidden):\n",
        "         \n",
        "        # shape(text) = [B, T]\n",
        "        \n",
        "        # If vanilla RNN:\n",
        "            # shape(prev_hidden) = [1, B, D] where 1 = num_layers*num_directions\n",
        "        # If LSTM:\n",
        "            # prev_hidden is a tuple of previous hidden states and cell states: (ALL_HIDDEN_STATES, ALL_CELL_STATES)\n",
        "            # shape(ALL_HIDDEN_STATES)=shape(ALL_CELL_STATES) = [1, B, D] where 1 = num_layers*num_directions\n",
        "            \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        # shape(embedded) = [B, T, E]\n",
        "        \n",
        "        all_hidden, last_hidden = self.rnn(embedded, prev_hidden)        \n",
        "        # shape(all_hidden) = [B, T, D]\n",
        "        # shape(last_hidden) = [num layers, B, D]\n",
        "        # For a LSTM:\n",
        "            # last_hidden = (h_t, c_t)\n",
        "            # shape(h_t = c_t) = [num_layers, B, D]\n",
        "            \n",
        "        if self.variant == \"lstm\" or self.variant == \"my_lstm\":\n",
        "            last_hidden = last_hidden[0]\n",
        "        \n",
        "        # Take all hidden states to produce an output word per time step\n",
        "        \n",
        "        logits = self.fc(self.dropout(last_hidden))\n",
        "        # shape(logits) = [B, O]\n",
        "            \n",
        "        return logits, last_hidden\n",
        "\n",
        "class My_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "  \n",
        "    \n",
        "    def forward(self, x, prev_hidden):\n",
        "\n",
        "  \n",
        "        return hidden_states, (h_t, c_t)\n",
        " \n",
        "my_lstm = RNN(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM,\n",
        "            DROPOUT, \n",
        "            PAD_IDX,\n",
        "            variant=\"my_lstm\")\n",
        "\n",
        "lstm = RNN(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM,\n",
        "            DROPOUT, \n",
        "            PAD_IDX,\n",
        "            variant=\"lstm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqGMo0IC5lDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(my_lstm.parameters())\n",
        "train(my_lstm, train_iterator, valid_iterator, optimizer, criterion, is_lstm=True, force_stop=True)\n",
        "\n",
        "optimizer = optim.Adam(lstm.parameters())\n",
        "train(lstm, train_iterator, valid_iterator, optimizer, criterion, is_lstm=True, force_stop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_V-UVMj6brx",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 3: Predicting Humor in Edited News Headlines\n",
        "\n",
        "Please read and follow the [homework requirements](https://docs.google.com/document/d/1zupXePA0SOlBJ39mfd5fnTghOAJx5AdbsPoo_5sa6co/edit?usp=sharing).\n",
        "\n",
        "Develop a regression model (different from Assignment 2) to assess Humor in Edited News Headlines (in English). Use the same dataset as you used in the previous assignment. Details on the task are outlined here: https://competitions.codalab.org/competitions/20970. \n",
        "\n",
        "You can use the code from lab sessions or write your own code. Final results are to be submitted to the online competition platform (submit to Post-Evaluation-Task-1). Pay particular attention to the submission format. The evaluation of results is done automatically by the platform. The result will be displayed publicly on the leaderboard. You will be able to download the output log with the evaluation results that you will have to attach to your homework submission.\n",
        "\n",
        "Submit the code, your user name (as displayed on the leaderboard), scoring output log from your competition account and a short report (150 words) answering the following questions:  \n",
        "\n",
        "1. Briefly describe your model. How the model is different to the model from Assignment 2? Why did you pick up this new model?\n",
        "2. What is the final set hyperparameters? How did you tune them?\n",
        "3. Did you manage to improve your previous performance? Report the previous and the current performance. If yes or no, please explain why."
      ]
    }
  ]
}