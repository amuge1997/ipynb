{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CSE 140 - Assignment 5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDo6Mz1MoJxR"
      },
      "source": [
        "# Assignment 5\n",
        "\n",
        "Turn in the assignment via Canvas.\n",
        "\n",
        "To write legible answers you will need to be familiar with both [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) and [Latex](https://www.latex-tutorial.com/tutorials/amsmath/)\n",
        "\n",
        "Before you turn this problem in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Runtime→→Restart runtime) and then run all cells (in the menubar, select Runtime→→Run All).\n",
        "\n",
        "Make sure you fill in any place that says \"YOUR CODE HERE\" or \"YOUR ANSWER HERE\", as well as your name below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOfazHDtoJxS"
      },
      "source": [
        "NAME = \"\"\n",
        "STUDENT_ID = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfM_a6Lxq4_m"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "You will model a small bayesian network that represents the relationship between yellow fingers, smoking, cancer, radiation, solar flares, and using a microwave.\n",
        "\n",
        "In this model, smoking can cause yellow fingers and cancer. Solar flares and making microwave popcorn can cause radiation, and radiation can cause cancer as well. The prior probability of smoking P(S) is 0.3. The prior probability of solar flares P(F) is 0.8. The prior probability of using the microwave is P(M) is 0.9.\n",
        "\n",
        "The conditional probability table for radiation is:\n",
        "\n",
        ">F | M | P(R)\n",
        ">--- | --- | ---\n",
        ">0 | 0 | 0.1\n",
        ">0 | 1 | 0.2\n",
        ">1 | 0 | 0.4\n",
        ">1 | 1 | 0.3\n",
        "\n",
        "The conditional probability table for cancwe is:\n",
        "\n",
        ">S | R | P(C)\n",
        ">--- | --- | ---\n",
        ">0 | 0 | 0.1\n",
        ">0 | 1 | 0.2\n",
        ">1 | 0 | 0.2\n",
        ">1 | 1 | 0.5\n",
        "\n",
        "The conditional probability table for yellow fingers is\n",
        "\n",
        ">S  | P(Y)\n",
        ">--- | ---\n",
        ">0 |  0.1\n",
        ">1 | 0.9\n",
        "\n",
        "Answer the following questions:\n",
        "1. Draw the graphical representation of the Bayesian Network (no need to include CPTs).\n",
        "\n",
        "2. What is the prior probability of cancer?\n",
        "\n",
        "3. What is the probability of smoking given cancer?\n",
        "\n",
        "4. What is the probability of smoking given cancer and radiation?\n",
        "\n",
        "5. Are solar flares and using the microwave independent given cancer?\n",
        "\n",
        "6. What is the probability of cancer is you never use a microwave?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dffe23Suk-5"
      },
      "source": [
        "[YOUR ANSWER HERE. ADD MORE CELLS AS NEEDED.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "5de1d899a516090395f300c2ecc737f0",
          "grade": false,
          "grade_id": "cell-f12bff039a5016a3",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "6q2qG3e3BwDf"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Using MDP, model the spread of COVID-19 for creating a simulator, under certain assumptions in a hypothetical situation. In the simulation, the inhabitants of a town move around as they go about their regular business.\n",
        "\n",
        "In the simulation, there are 2 types of people in the town, a **carrier** or **non-carrier** of the Coronavirus. The virus spreads via droplets when an infected person comes in close contact (within 6 feets) of another person who has the virus, a carrier. Non-carriers cannot transmit the disease. Associated with these two types are 5 different states that a person can be in. A carrier can be in one of the following 2 states: infected (but not sick i.e. asymptomatic), or sick. A non-carrier can be in 3 states: unexposed, dead, or immune.\n",
        "\n",
        "The states (with a distinct color representing the state in the MDP) are described as the following:\n",
        "\n",
        "\n",
        "*   **Unexposed** (BLUE): The people who haven't encountered the virus at all so far.\n",
        "*   **Infected** (ORANGE): People who have the virus but have no symptoms of the disease. An unexposed person has an 80% chance of getting infected on coming in contact with a person carrying the virus. An infected person may get sick with symptoms in 5 days or stay infected (contagious) for 15 days and develop immunity after that.\n",
        "*   **Sick** (RED): Symptomatic people who have the virus and are sick. Of those infected individuals, 50% get sick with the disease. In 10 days, a sick person may recover completely (98%) and develop lifelong immunity, or may die (2%).\n",
        "*   **Dead** (GRAY): People who die from the disease. 2% of the sick die.\n",
        "*   **Immune** (GREEN): There are two ways of getting to this state: 1) People who got sick with COVID-19, and recovered and thereby, have developed lifelong immunity, 2) People who were infected but didn't develop symptoms and became immune once they stopped being contagious.\n",
        "\n",
        "\n",
        "Let's assume that, the cost for running 1 day of simulation is 1 standard processing cycle of a specific computer. On day 1 of the simulation, a (random) person returned to the town from elsewhere and is sick with the virus. Unexposed people who come in contact with this carrier as they travel between home and work may become infected with the virus. There is an 80% chance of contracting the virus (and changing state to infected) on contact with a carrier. We assume that this is the only means by which an unexposed person may get infected by the virus, and they are infected immediately. To summarize, the following transitions between states are possible:\n",
        "\n",
        "*   Unexposed → Infected (happens immediately on the same day after each contact with 80% chance)\n",
        "*   Infected → Sick (after 5 days of getting infected with 50% chance)\n",
        "*   Infected → Immune (after 15 days of getting infected with 50% chance)\n",
        "*   Sick → Immune (after 10 days of getting sick with 98% chance)\n",
        "*   Sick → Dead (after 10 days of getting sick with 2% chance)\n",
        "\n",
        "a) Draw the MDP graphically\n",
        "  - A good way to do this is through [Google Drawings](https://docs.google.com/drawings)\n",
        "  - When you're done you can embed it in the jupyter notebook using markdown syntax \\!\\[alt-text\\]\\(url\\)\n",
        "  - To get the URL for your image in Google Draw goto File->Publish to the web...->Embed and copy the src portion of the html tag\n",
        "\n",
        "b) Create the transition probabilities matrix/table for the MDP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4IXl8khyAHk"
      },
      "source": [
        "[YOUR ANSWER HERE]\n",
        "\n",
        "a)\n",
        "\n",
        "\n",
        "b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N8K_Ls1A87g"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Solve the following  MDP using both Value Iteration and Policy Iteration algorithms.\n",
        "\n",
        "Neo is a freelance computer programmer, and his aim in life is to earn as much money as he can by writing more code. He has three possible states in his daily professional life as below.\n",
        "\n",
        "1.   **Productive** - He can write programming codes more efficiently.\n",
        "2.   **Exhausted** - He is too tired to think well in order to write efficient programming codes.\n",
        "3.   **Fit** - He is physically and mentally in a great state to write code and solve programming problems.\n",
        "\n",
        "\n",
        "When Neo is **Exhausted**, he can choose one of three actions: (1) Keep **Coding**, (2) do some physical **Exercise**, or (3) get some **Rest**.\n",
        "\n",
        "If he chooses to do more coding, he remains in the **Exhausted** state with the certainty of getting a \\$20 reward. If he decides to get Rest, he has 80% chance of moving to the next state, **Productive**, and a 20% chance of staying **Exhausted**.\n",
        "If he doesn’t want to get Rest, he may go to the gym and do some physical Exercise. This gives him a 50% chance of entering the **Productive** state and a 50% chance of staying **Exhausted** state. However, he needs to pay for the gym, so this choice results in a -\\$10 reward.\n",
        "\n",
        "When Neo becomes **Productive**, he can write programs more efficiently. From there, he has an 80% chance of getting **Exhausted** again with earning a \\$40 reward, and a 20% chance of staying **Productive** while earning a \\$30 reward.\n",
        "Sometimes, when he is **Productive**, he wants to do some physical Exercise. When he Exercises in this state, he enjoys it very much and gets 100% **Fit** physically and mentally. However, he needs to pay \\$10 for it.\n",
        "\n",
        "Once Neo reaches the state **Fit**, he is fully-committed to earning more money by more Coding. Because he is in such a great state physically and mentally, he can write programs very efficiently. In this state, he earns a \\$100 reward, and he keeps writing code until he is **Exhausted** again.\n",
        "\n",
        "Use MDP to find the best policy to maximize his earnings/rewards over time.\n",
        "\n",
        "1. Draw the MDP graphically\n",
        "  - A good way to do this is through [Google Drawings](https://docs.google.com/drawings)\n",
        "  - When you're done you can embed it in the jupyter notebook using markdown syntax \\!\\[alt-text\\]\\(url\\)\n",
        "  - To get the URL for your image in Google Draw goto File->Publish to the web...->Embed and copy the src portion of the html tag\n",
        "\n",
        "2. Using a discount factor of 0.86, solve the MDP using value iteration algorithm (until the values have become reasonably stable). You should start with the values set to zero. You should show both the optimal policy and the optimal values.\n",
        "\n",
        "3. Using a discount factor of 0.86, solve the MDP using policy iteration algorithm (until you have complete convergence). You should start with the policy that always does **Coding**.\n",
        "\n",
        "4. Change the MDP in three different ways: by changing the discount factor, changing the transition probabilities for a single action from a single state, and by changing a reward for a single action at a single state. Each of these changes should be performed separately starting at the original MDP, resulting in three new MDPs (which you do not have to draw), each of which is different from the original MDP in a single way. In each case, the change should be so that the optimal policy changes, and you should state what the optimal policy becomes and give a short argument for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jckgTbK2qESC"
      },
      "source": [
        "[YOUR ANSWER HERE. ADD MORE CELLS AS NEEDED.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQJg8PvNcRGt"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "### Markov Decision Process (MDP) Toolbox for Python\n",
        "\n",
        "One useful Python module for solving these types of problems is called *MDP toolbox* (pymdptoolbox). The MDP toolbox provides classes and functions for the resolution of descrete-time Markov Decision Processes. The list of algorithms that have been implemented includes backwards induction, linear programming, policy iteration, q-learning and value iteration along with several variations. In the next cell you'll see how to load this module into a Jupyter notebook running in Colab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WrBebxhcJID",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5b55bed8-f188-4df7-9c87-09b8cb83f184"
      },
      "source": [
        "!pip install pymdptoolbox"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymdptoolbox in /usr/local/lib/python3.6/dist-packages (4.0b3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pymdptoolbox) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pymdptoolbox) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTl9nuj3Zhr5"
      },
      "source": [
        "A simple example code is given below. For details about the example, check the documentation for Python MDP Toolbox (pymdptoolbox) from this [link](https://pymdptoolbox.readthedocs.io/en/latest/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vKT0UTgZik-"
      },
      "source": [
        "import numpy as np\n",
        "import mdptoolbox\n",
        "import mdptoolbox.example\n",
        "np.random.seed(0) # Needed to get the output below\n",
        "P, R = mdptoolbox.example.rand(2, 2)\n",
        "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
        "pi.run()\n",
        "print(pi.policy)\n",
        "print(pi.iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QW-tv3L3x99"
      },
      "source": [
        "## Solving a MDP using Python MDP Toolbox\n",
        "\n",
        "First read the documentation for Python MDP Toolbox (pymdptoolbox) from this [link](https://pymdptoolbox.readthedocs.io/en/latest/index.html) and try to understand how it works.\n",
        "\n",
        "Then solve the MDP from Question 3 using both value iteration and policy iteration algorithms from the Python MDP Toolbox.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfem6k9-ceSg"
      },
      "source": [
        "# Write your code here. Add more cells as needed."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gmZOG5tygH2"
      },
      "source": [
        "## Question 5 - Bonus [up to an additional 20% on this assignment]\n",
        "\n",
        "### Convert question (3) into a Reinforcement Learning Problem and solve it\n",
        "\n",
        "Write a simulator that you can use in order to run the exploration stage of the RL (observing an state and taking a specific action --> simulator returns a reward value and a new state). Then solve the RL problem:\n",
        "\n",
        "1. Using Model-based RL approach\n",
        "\n",
        "2. Using Model-free RL approach\n",
        "\n",
        "in order to find 1) the optimum value of each of the states and 2) the optimum policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r5GHJyJ0LWA"
      },
      "source": [
        "# Write your code here. Add more cells as needed."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}